{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "from nltk.stem import PorterStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================\n",
    "def ps_stem(word_list):\n",
    "    ps = PorterStemmer()\n",
    "    p=[]\n",
    "    for w in word_list:\n",
    "        p.append(ps.stem(w))\n",
    "    return(p)\n",
    "#==============================\n",
    "def process_files(path, filenames):\n",
    "    file_to_terms = {}\n",
    "    for file in filenames:\n",
    "        pattern = re.compile('[\\W_\\d]+')\n",
    "        file_to_terms[file] = open(path+file, 'r', encoding=\"ANSI\").read().lower();\n",
    "        file_to_terms[file] = pattern.sub(' ',file_to_terms[file])\n",
    "        re.sub(r'[\\W_\\d]+','', file_to_terms[file])\n",
    "        file_to_terms[file] = file_to_terms[file].split()\n",
    "        file_to_terms[file] = ps_stem(file_to_terms[file]) #ps_stemmer\n",
    "    return file_to_terms\n",
    "\n",
    "def index_one_file(termlist):\n",
    "    fileIndex = {}\n",
    "    for index, word in enumerate(termlist):\n",
    "        if word in fileIndex.keys():\n",
    "            fileIndex[word].append(index)\n",
    "        else:\n",
    "            fileIndex[word] = [index]\n",
    "    return fileIndex\n",
    "\n",
    "def make_indices(termlists):\n",
    "    total = {}\n",
    "    for filename in termlists.keys():\n",
    "        total[filename] = index_one_file(termlists[filename])\n",
    "    return total\n",
    "\n",
    "def fullIndex(regdex):\n",
    "    total_index = {}\n",
    "    for filename in regdex.keys():\n",
    "        for word in regdex[filename].keys():\n",
    "            if word in total_index.keys():\n",
    "                if filename in total_index[word].keys():\n",
    "                    total_index[word][filename].extend(regdex[filename][word][:])\n",
    "                else:\n",
    "                    total_index[word][filename] = regdex[filename][word]\n",
    "            else:\n",
    "                total_index[word] = {filename: regdex[filename][word]}\n",
    "    return total_index\n",
    "#===========================\n",
    "def get_filenames(path):\n",
    "    filenames=[]\n",
    "    for f in os.listdir(path):\n",
    "        filenames.append(f)\n",
    "    filenames.sort(key = lambda i:int(re.search(r'(\\d+)',i).group())) #照檔名數字排\n",
    "    return(filenames)\n",
    "\n",
    "def inv_dict_sort(inv_index):\n",
    "    inv_index_sort={}\n",
    "    for key in sorted(inv_index.keys()):\n",
    "        inv_index_sort[key] = inv_index[key]\n",
    "    return(inv_index_sort)\n",
    "\n",
    "#for add document\n",
    "def sort_dict_by_docname(d):\n",
    "    keys = list(d)\n",
    "    keys.sort(key = lambda i:int(re.search(r'(\\d+)',i).group())) #照檔名數字排\n",
    "    temp_dict={}\n",
    "    for i in keys:\n",
    "        temp_dict[i]=d[i]\n",
    "    return(temp_dict)\n",
    "#===========================\n",
    "\n",
    "#Remove document\n",
    "def rm_doc(path, doclist, inv_index):\n",
    "    for docname in doclist:\n",
    "        item = make_indices(process_files(path, [docname]))\n",
    "        terms = item[docname].keys()\n",
    "        for i in terms:\n",
    "            inv_index[i].pop(docname, None)\n",
    "    return(inv_index)\n",
    "\n",
    "#Add document\n",
    "def add_doc(path, doclist, inv_index):\n",
    "    for docname in doclist:\n",
    "        item = make_indices(process_files(path, [docname]))\n",
    "        terms = item[docname].keys()\n",
    "        for i in terms:\n",
    "            try:\n",
    "                inv_index[i].update( {docname : item[docname][i]} )\n",
    "            except:\n",
    "                inv_index[i]={docname : item[docname][i]}\n",
    "            inv_index[i] = sort_dict_by_docname(inv_index[i])  \n",
    "    return(inv_index)\n",
    "#===========================\n",
    "\n",
    "def one_word_query(word, invertedIndex):\n",
    "    ps = PorterStemmer()\n",
    "    pattern = re.compile('[\\W_\\d]+')\n",
    "    word = pattern.sub(' ',word).lower()\n",
    "    word = ps.stem(word) #stemmer\n",
    "    if word in invertedIndex.keys():\n",
    "        return [filename for filename in invertedIndex[word].keys()]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def free_text_query(string):\n",
    "    pattern = re.compile('[\\W_]+')\n",
    "    string = pattern.sub(' ',string)\n",
    "    result = []\n",
    "    for word in string.split():\n",
    "        result += one_word_query(word,inv_index)\n",
    "    return list(set(result))\n",
    "\n",
    "#===========================    \n",
    "def AND(list1, list2, pt=False):\n",
    "    answer=[]\n",
    "    i=iter(list1)\n",
    "    j=iter(list2)\n",
    "    if list1 and list2:\n",
    "        p1=next(i)\n",
    "        p2=next(j)\n",
    "        while True:\n",
    "            try:\n",
    "                #正規表達式\n",
    "                n1 = int(re.search(r'(\\d+)',p1).group())\n",
    "                n2 = int(re.search(r'(\\d+)',p2).group())\n",
    "                if(pt):\n",
    "                    print('p1 %s' % p1)\n",
    "                    print('p2 %s' % p2)\n",
    "                    print('---')\n",
    "                if(n1==n2):\n",
    "                    answer.append(p1)\n",
    "                    p1=next(i)\n",
    "                    p2=next(j)\n",
    "                elif(n1<n2):\n",
    "                    p1=next(i)\n",
    "                else:\n",
    "                    p2=next(j)\n",
    "            except StopIteration:\n",
    "                break\n",
    "    return(answer)\n",
    "#=========================== \n",
    "\n",
    "def AND_EASY(list1,list2):\n",
    "    ListA = list()\n",
    "    for x in list1:\n",
    "        for y in list2:\n",
    "            if(x == y):\n",
    "                ListA.append(x)\n",
    "    \n",
    "    return ListA    \n",
    "\n",
    "#=========================== \n",
    "def phrase_query(string, invertedIndex):\n",
    "    ps = PorterStemmer()\n",
    "    pattern = re.compile('[\\W_\\d]+')\n",
    "    string = pattern.sub(' ',string)\n",
    "    listOfLists, result = [],[]\n",
    "    for word in string.split():\n",
    "        word = ps.stem(word) #stemmer\n",
    "        listOfLists.append(one_word_query(word, invertedIndex))\n",
    "    setted = set(listOfLists[0]).intersection(*listOfLists)\n",
    "    for filename in setted:\n",
    "        temp = []\n",
    "        for word in string.split():\n",
    "            word = ps.stem(word) #stemmer\n",
    "            try:\n",
    "                temp.append(invertedIndex[word][filename][:])\n",
    "            except:\n",
    "                continue\n",
    "        for i in range(len(temp)):\n",
    "            for ind in range(len(temp[i])):\n",
    "                temp[i][ind] -= i\n",
    "        if set(temp[0]).intersection(*temp):\n",
    "            result.append(filename)\n",
    "    #return rankResults(result, string)\n",
    "    result.sort(key = lambda i:int(re.search(r'(\\d+)',i).group())) #照檔名數字排\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#主執行區\n",
    "path = './data/txt/'\n",
    "filenames = get_filenames(path)\n",
    "inv_index = fullIndex(make_indices(process_files(path, filenames)))\n",
    "inv_index = inv_dict_sort(inv_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#移除文件\n",
    "path = './data/txt/'\n",
    "doclist = ['d (3).txt','d (12).txt']\n",
    "inv_index = rm_doc(path, doclist, inv_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加入文件\n",
    "path = './data/txt/'\n",
    "doclist = ['d (3).txt','d (12).txt']\n",
    "inv_index = add_doc(path, doclist, inv_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d (1).txt': [115, 122, 194],\n",
       " 'd (2).txt': [87],\n",
       " 'd (3).txt': [14, 26, 80, 127, 134, 147, 265, 300, 331, 362, 418, 458],\n",
       " 'd (12).txt': [37],\n",
       " 'd (47).txt': [352],\n",
       " 'd (116).txt': [228, 246, 260],\n",
       " 'd (158).txt': [24],\n",
       " 'd (184).txt': [447],\n",
       " 'd (185).txt': [296],\n",
       " 'd (188).txt': [463],\n",
       " 'd (204).txt': [348],\n",
       " 'd (245).txt': [811],\n",
       " 'd (250).txt': [212, 259],\n",
       " 'd (251).txt': [40],\n",
       " 'd (252).txt': [6, 264],\n",
       " 'd (253).txt': [217, 357],\n",
       " 'd (254).txt': [16],\n",
       " 'd (296).txt': [228, 1067],\n",
       " 'd (317).txt': [41],\n",
       " 'd (385).txt': [57],\n",
       " 'd (391).txt': [112],\n",
       " 'd (418).txt': [103, 217, 471, 677, 826],\n",
       " 'd (419).txt': [147, 167, 192],\n",
       " 'd (504).txt': [507],\n",
       " 'd (508).txt': [175],\n",
       " 'd (545).txt': [208],\n",
       " 'd (585).txt': [179],\n",
       " 'd (608).txt': [331],\n",
       " 'd (634).txt': [565],\n",
       " 'd (640).txt': [259],\n",
       " 'd (652).txt': [514, 553],\n",
       " 'd (663).txt': [517],\n",
       " 'd (667).txt': [551, 572],\n",
       " 'd (677).txt': [575],\n",
       " 'd (683).txt': [259],\n",
       " 'd (718).txt': [9],\n",
       " 'd (754).txt': [710, 761, 836],\n",
       " 'd (755).txt': [500],\n",
       " 'd (850).txt': [455, 467],\n",
       " 'd (886).txt': [68],\n",
       " 'd (907).txt': [291],\n",
       " 'd (910).txt': [131],\n",
       " 'd (924).txt': [38, 73],\n",
       " 'd (936).txt': [176],\n",
       " 'd (960).txt': [819],\n",
       " 'd (968).txt': [282],\n",
       " 'd (987).txt': [306],\n",
       " 'd (1024).txt': [372, 585],\n",
       " 'd (1032).txt': [129],\n",
       " 'd (1035).txt': [346],\n",
       " 'd (1047).txt': [1123],\n",
       " 'd (1050).txt': [395, 494],\n",
       " 'd (1052).txt': [32, 70, 131],\n",
       " 'd (1058).txt': [123],\n",
       " 'd (1060).txt': [1363],\n",
       " 'd (1081).txt': [113, 489],\n",
       " 'd (1083).txt': [143],\n",
       " 'd (1142).txt': [27],\n",
       " 'd (1143).txt': [58],\n",
       " 'd (1144).txt': [50, 98, 103, 118, 147, 189, 251],\n",
       " 'd (1145).txt': [101],\n",
       " 'd (1146).txt': [581, 590],\n",
       " 'd (1147).txt': [14, 97, 120, 127],\n",
       " 'd (1148).txt': [87],\n",
       " 'd (1160).txt': [459],\n",
       " 'd (1162).txt': [134],\n",
       " 'd (1164).txt': [347]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_index['constant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#單字查詢\n",
    "query = 'p2p'\n",
    "print(one_word_query(query,inv_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d (147).txt', 'd (148).txt', 'd (149).txt', 'd (150).txt', 'd (151).txt', 'd (159).txt', 'd (167).txt', 'd (168).txt', 'd (169).txt', 'd (170).txt', 'd (171).txt', 'd (172).txt', 'd (303).txt', 'd (304).txt', 'd (305).txt', 'd (306).txt', 'd (308).txt', 'd (309).txt', 'd (1010).txt', 'd (1011).txt', 'd (1012).txt', 'd (1015).txt', 'd (1062).txt', 'd (1063).txt', 'd (1064).txt', 'd (1070).txt', 'd (1074).txt']\n"
     ]
    }
   ],
   "source": [
    "#連續字查詢\n",
    "q1 = phrase_query('skyline query',inv_index)\n",
    "print(q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q1_len:891\n",
      "q2_len:878\n",
      "AND_len:749\n",
      "AND_EASY : --- 0.024001121520996094 seconds ---\n",
      "\n",
      "\n",
      "q1_len:891\n",
      "q2_len:878\n",
      "AND_len:749\n",
      "AND : --- 0.004000186920166016 seconds ---\n",
      "\n",
      "\n",
      "q1_len:878\n",
      "q2_len:891\n",
      "AND_len:749\n",
      "AND_HASH : --- 0.0010001659393310547 seconds ---\n",
      "\n",
      "\n",
      "q1_len:878\n",
      "q2_len:891\n",
      "OR_len:1020\n",
      "OR_HASH : --- 0.0 seconds ---\n",
      "\n",
      "\n",
      "q1_len:878\n",
      "q2_len:891\n",
      "NOT_len:129\n",
      "NOT_HASH : --- 0.0010001659393310547 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#===AND_EASY===\n",
    "start_time = time.time()\n",
    "\n",
    "q1 = one_word_query('this',inv_index)\n",
    "q2 = one_word_query('we',inv_index)\n",
    "intersection = AND_EASY(q1,q2)\n",
    "print('q1_len:%s' % len(q1))\n",
    "print('q2_len:%s' % len(q2))\n",
    "print('AND_len:%s' % len(intersection))\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"AND_EASY : --- %s seconds ---\\n\\n\" % (end_time - start_time))\n",
    "\n",
    "#===AND===\n",
    "start_time = time.time()\n",
    "\n",
    "q1 = one_word_query('this',inv_index)\n",
    "q2 = one_word_query('we',inv_index)\n",
    "intersection = AND(q1,q2)\n",
    "print('q1_len:%s' % len(q1))\n",
    "print('q2_len:%s' % len(q2))\n",
    "print('AND_len:%s' % len(intersection))\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"AND : --- %s seconds ---\\n\\n\" % (end_time - start_time))\n",
    "\n",
    "#===AND_HASH===\n",
    "start_time = time.time()\n",
    "\n",
    "q1 = set(one_word_query('we',inv_index))\n",
    "q2 = set(one_word_query('this',inv_index))\n",
    "intersection = list(q1 & q2) # '&' operator is used for set intersection\n",
    "print('q1_len:%s' % len(q1))\n",
    "print('q2_len:%s' % len(q2))\n",
    "print('AND_len:%s' % len(intersection))\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"AND_HASH : --- %s seconds ---\\n\\n\" % (end_time - start_time))\n",
    "\n",
    "#===OR_HASH===\n",
    "start_time = time.time()\n",
    "\n",
    "q1 = set(one_word_query('we',inv_index))\n",
    "q2 = set(one_word_query('this',inv_index))\n",
    "union = list(q1 | q2) # '&' operator is used for set intersection\n",
    "print('q1_len:%s' % len(q1))\n",
    "print('q2_len:%s' % len(q2))\n",
    "print('OR_len:%s' % len(union))\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"OR_HASH : --- %s seconds ---\\n\\n\" % (end_time - start_time))\n",
    "\n",
    "#===NOT_HASH===\n",
    "start_time = time.time()\n",
    "\n",
    "q1 = set(one_word_query('we',inv_index))\n",
    "q2 = set(one_word_query('this',inv_index))\n",
    "Complement = list(q1 - q2) # '&' operator is used for set intersection\n",
    "print('q1_len:%s' % len(q1))\n",
    "print('q2_len:%s' % len(q2))\n",
    "print('NOT_len:%s' % len(Complement))\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"NOT_HASH : --- %s seconds ---\" % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496\n"
     ]
    }
   ],
   "source": [
    "query = free_text_query('base-station')\n",
    "print(len(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496\n",
      "454\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "def q(text):\n",
    "    text = set(one_word_query(text,inv_index))\n",
    "    return text\n",
    "\n",
    "query = q('base') | q('station')\n",
    "print(len(query))\n",
    "\n",
    "query = q('base') - q('station')\n",
    "print(len(query))\n",
    "\n",
    "query = q('base') & q('station')\n",
    "print(len(query))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
